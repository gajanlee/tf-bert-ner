{
    "model_config": {
        "init_checkpoint": "/etc/bert/chinese_L-12_H-768_A-12/bert_model.ckpt",
        "vocab_file": "/etc/bert/chinese_L-12_H-768_A-12/vocab.txt",
        "output_dir": "/etc/bert/ner/output",
        "data_dir": "./data",
        "do_train": false,
        "do_eval": false,
        "do_predict": false,
        "max_seq_length": 64,
        "train_batch_size": 16,
        "eval_batch_size": 32,
        "predict_batch_size": 32,
        "service_batch_size": 1,
        "learning_rate": 2e-5,
        "num_train_epochs": 3,
        "warmup_proportion": 0.1,
        "save_summary_steps": 50,
        "save_checkpoints_steps": 500
    },
    "bert_config": {
        "attention_probs_dropout_prob": 0.1, 
        "directionality": "bidi", 
        "hidden_act": "gelu", 
        "hidden_dropout_prob": 0.1, 
        "hidden_size": 768, 
        "initializer_range": 0.02, 
        "intermediate_size": 3072, 
        "max_position_embeddings": 512, 
        "num_attention_heads": 12, 
        "num_hidden_layers": 12, 
        "pooler_fc_size": 768, 
        "pooler_num_attention_heads": 12, 
        "pooler_num_fc_layers": 3, 
        "pooler_size_per_head": 128, 
        "pooler_type": "first_token_transform", 
        "type_vocab_size": 2, 
        "vocab_size": 21128
    },
    "labels": ["_X", "B-PER", "I-PER", "O"],
    "label_names": [],
    "logger_file": "/etc/bert/ner/output/ner.log"
}
